> ## Documentation Index
> Fetch the complete documentation index at: https://docs.openhands.dev/llms.txt
> Use this file to discover all available pages before exploring further.

# Context Condenser

> Manage agent memory by condensing conversation history to save tokens.

export const path_to_script_0 = "examples/01_standalone_sdk/14_context_condenser.py"

> A ready-to-run example is available [here](#ready-to-run-example)!

## What is a Context Condenser?

A **context condenser** is a crucial component that addresses one of the most persistent challenges in AI agent development: managing growing conversation context efficiently. As conversations with AI agents grow longer, the cumulative history leads to:

* **üí∞ Increased API Costs**: More tokens in the context means higher costs per API call
* **‚è±Ô∏è Slower Response Times**: Larger contexts take longer to process
* **üìâ Reduced Effectiveness**: LLMs become less effective when dealing with excessive irrelevant information

The context condenser solves this by intelligently summarizing older parts of the conversation while preserving essential information needed for the agent to continue working effectively.

## Default Implementation: `LLMSummarizingCondenser`

OpenHands SDK provides `LLMSummarizingCondenser` as the default condenser implementation. This condenser uses an LLM to generate summaries of conversation history when it exceeds the configured size limit.

### How It Works

When conversation history exceeds a defined threshold, the LLM-based condenser:

1. **Keeps recent messages intact** - The most recent exchanges remain unchanged for immediate context
2. **Preserves key information** - Important details like user goals, technical specifications, and critical files are retained
3. **Summarizes older content** - Earlier parts of the conversation are condensed into concise summaries using LLM-generated summaries
4. **Maintains continuity** - The agent retains awareness of past progress without processing every historical interaction

<img className="block dark:hidden" src="https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_light_mode.png?fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=dc85afa9f2821b1f7a305ad1406ab3d9" alt="Light mode interface" data-og-width="3061" width="3061" data-og-height="1677" height="1677" data-path="sdk/guides/assets/condenser_overview_light_mode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_light_mode.png?w=280&fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=2743fa2ff5fdbe6e44a3e5e137894e8d 280w, https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_light_mode.png?w=560&fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=667733594c803b8a452121e46c1d08b2 560w, https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_light_mode.png?w=840&fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=c826d110a2cc6367b50ddbf0802de39f 840w, https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_light_mode.png?w=1100&fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=6b5d4c73b65516317f38a393ed31cf74 1100w, https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_light_mode.png?w=1650&fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=c4e0ed509648afd5a5f44ffe30159294 1650w, https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_light_mode.png?w=2500&fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=ef1e72e28968fe459c5a7f48c2ab9ad9 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_dark_mode.png?fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=80dc8ab896f54e426af75b31ac159be0" alt="Dark mode interface" data-og-width="3061" width="3061" data-og-height="1677" height="1677" data-path="sdk/guides/assets/condenser_overview_dark_mode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_dark_mode.png?w=280&fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=4141703d3d21052895fd8c4b7e25c22c 280w, https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_dark_mode.png?w=560&fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=89db1cd66f4f9f621f6aff81bd04748d 560w, https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_dark_mode.png?w=840&fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=f20439849a931cd1400d6635c5f89043 840w, https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_dark_mode.png?w=1100&fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=83ca87a8fed893c2aa37313e6be52a82 1100w, https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_dark_mode.png?w=1650&fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=ab5f8a0e59ff30d114d65a63e7b7fc73 1650w, https://mintcdn.com/allhandsai/hwL6uO0ZqM-lmlYG/sdk/guides/assets/condenser_overview_dark_mode.png?w=2500&fit=max&auto=format&n=hwL6uO0ZqM-lmlYG&q=85&s=e16620e66605ea1b5218b34578a7cd2d 2500w" />

This approach achieves remarkable efficiency gains:

* Up to **2x reduction** in per-turn API costs
* **Consistent response times** even in long sessions
* **Equivalent or better performance** on software engineering tasks

Learn more about the implementation and benchmarks in our [blog post on context condensation](https://openhands.dev/blog/openhands-context-condensensation-for-more-efficient-ai-agents).

### Extensibility

The `LLMSummarizingCondenser` extends the `RollingCondenser` base class, which provides a framework for condensers that work with rolling conversation history. You can create custom condensers by extending base classes ([source code](https://github.com/OpenHands/software-agent-sdk/blob/main/openhands-sdk/openhands/sdk/context/condenser/base.py)):

* **`RollingCondenser`** - For condensers that apply condensation to rolling history
* **`CondenserBase`** - For more specialized condensation strategies

This architecture allows you to implement custom condensation logic tailored to your specific needs while leveraging the SDK's conversation management infrastructure.

### Setting Up Condensing

Create a `LLMSummarizingCondenser` to manage the context.
The condenser will automatically truncate conversation history when it exceeds max\_size, and replaces the dropped events with an LLM-generated summary.

This condenser triggers when there are more than `max_context_length` events in
the conversation history, and always keeps the first `keep_first` events (system prompts,
initial user messages) to preserve important context.

```python focus={3-4} icon="python" theme={null}
from openhands.sdk.context import LLMSummarizingCondenser

condenser = LLMSummarizingCondenser(
    llm=llm.model_copy(update={"usage_id": "condenser"}), max_size=10, keep_first=2
)

# Agent with condenser
agent = Agent(llm=llm, tools=tools, condenser=condenser)
```

### Ready-to-run example

<Note>
  This example is available on GitHub: [examples/01\_standalone\_sdk/14\_context\_condenser.py](https://github.com/OpenHands/software-agent-sdk/blob/main/examples/01_standalone_sdk/14_context_condenser.py)
</Note>

Automatically condense conversation history when context length exceeds limits, reducing token usage while preserving important information:

```python icon="python" expandable examples/01_standalone_sdk/14_context_condenser.py theme={null}
"""
To manage context in long-running conversations, the agent can use a context condenser
that keeps the conversation history within a specified size limit. This example
demonstrates using the `LLMSummarizingCondenser`, which automatically summarizes
older parts of the conversation when the history exceeds a defined threshold.
"""

import os

from pydantic import SecretStr

from openhands.sdk import (
    LLM,
    Agent,
    Conversation,
    Event,
    LLMConvertibleEvent,
    get_logger,
)
from openhands.sdk.context.condenser import LLMSummarizingCondenser
from openhands.sdk.tool import Tool
from openhands.tools.file_editor import FileEditorTool
from openhands.tools.task_tracker import TaskTrackerTool
from openhands.tools.terminal import TerminalTool


logger = get_logger(__name__)

# Configure LLM
api_key = os.getenv("LLM_API_KEY")
assert api_key is not None, "LLM_API_KEY environment variable is not set."
model = os.getenv("LLM_MODEL", "anthropic/claude-sonnet-4-5-20250929")
base_url = os.getenv("LLM_BASE_URL")
llm = LLM(
    usage_id="agent",
    model=model,
    base_url=base_url,
    api_key=SecretStr(api_key),
)

# Tools
cwd = os.getcwd()
tools = [
    Tool(
        name=TerminalTool.name,
    ),
    Tool(name=FileEditorTool.name),
    Tool(name=TaskTrackerTool.name),
]

# Create a condenser to manage the context. The condenser will automatically truncate
# conversation history when it exceeds max_size, and replaces the dropped events with an
#  LLM-generated summary. This condenser triggers when there are more than ten events in
# the conversation history, and always keeps the first two events (system prompts,
# initial user messages) to preserve important context.
condenser = LLMSummarizingCondenser(
    llm=llm.model_copy(update={"usage_id": "condenser"}), max_size=10, keep_first=2
)

# Agent with condenser
agent = Agent(llm=llm, tools=tools, condenser=condenser)

llm_messages = []  # collect raw LLM messages


def conversation_callback(event: Event):
    if isinstance(event, LLMConvertibleEvent):
        llm_messages.append(event.to_llm_message())


conversation = Conversation(
    agent=agent,
    callbacks=[conversation_callback],
    persistence_dir="./.conversations",
    workspace=".",
)

# Send multiple messages to demonstrate condensation
print("Sending multiple messages to demonstrate LLM Summarizing Condenser...")

conversation.send_message(
    "Hello! Can you create a Python file named math_utils.py with functions for "
    "basic arithmetic operations (add, subtract, multiply, divide)?"
)
conversation.run()

conversation.send_message(
    "Great! Now add a function to calculate the factorial of a number."
)
conversation.run()

conversation.send_message("Add a function to check if a number is prime.")
conversation.run()

conversation.send_message(
    "Add a function to calculate the greatest common divisor (GCD) of two numbers."
)
conversation.run()

conversation.send_message(
    "Now create a test file to verify all these functions work correctly."
)
conversation.run()

print("=" * 100)
print("Conversation finished. Got the following LLM messages:")
for i, message in enumerate(llm_messages):
    print(f"Message {i}: {str(message)[:200]}")

# Conversation persistence
print("Serializing conversation...")

del conversation

# Deserialize the conversation
print("Deserializing conversation...")
conversation = Conversation(
    agent=agent,
    callbacks=[conversation_callback],
    persistence_dir="./.conversations",
    workspace=".",
)

print("Sending message to deserialized conversation...")
conversation.send_message("Finally, clean up by deleting both files.")
conversation.run()

print("=" * 100)
print("Conversation finished with LLM Summarizing Condenser.")
print(f"Total LLM messages collected: {len(llm_messages)}")
print("\nThe condenser automatically summarized older conversation history")
print("when the conversation exceeded the configured max_size threshold.")
print("This helps manage context length while preserving important information.")

# Report cost
cost = conversation.conversation_stats.get_combined_metrics().accumulated_cost
print(f"EXAMPLE_COST: {cost}")
```

You can run the example code as-is.

<Note>
  The model name should follow the [LiteLLM convention](https://models.litellm.ai/): `provider/model_name` (e.g., `anthropic/claude-sonnet-4-5-20250929`, `openai/gpt-4o`).
  The `LLM_API_KEY` should be the API key for your chosen provider.
</Note>

<CodeGroup>
  <CodeBlock language="bash" filename="Bring-your-own provider key" icon="terminal" wrap>
    {`export LLM_API_KEY="your-api-key"\nexport LLM_MODEL="anthropic/claude-sonnet-4-5-20250929"  # or openai/gpt-4o, etc.\ncd software-agent-sdk\nuv run python ${path_to_script_0}`}
  </CodeBlock>

  <CodeBlock language="bash" filename="OpenHands Cloud" icon="terminal" wrap>
    {`# https://app.all-hands.dev/settings/api-keys\nexport LLM_API_KEY="your-openhands-api-key"\nexport LLM_MODEL="openhands/claude-sonnet-4-5-20250929"\ncd software-agent-sdk\nuv run python ${path_to_script_0}`}
  </CodeBlock>
</CodeGroup>

<Tip>
  **ChatGPT Plus/Pro subscribers**: You can use `LLM.subscription_login()` to authenticate with your ChatGPT account and access Codex models without consuming API credits. See the [LLM Subscriptions guide](/sdk/guides/llm-subscriptions) for details.
</Tip>

## Next Steps

* **[LLM Metrics](/sdk/guides/metrics)** - Track token usage reduction and analyze cost savings
