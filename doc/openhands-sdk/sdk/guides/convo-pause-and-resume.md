> ## Documentation Index
> Fetch the complete documentation index at: https://docs.openhands.dev/llms.txt
> Use this file to discover all available pages before exploring further.

# Pause and Resume

> Pause agent execution, perform operations, and resume without losing state.

export const path_to_script_0 = "examples/01_standalone_sdk/09_pause_example.py"

> A ready-to-run example is available [here](#ready-to-run-example)!

### Pausing Execution

Pause the agent from another thread or after a delay using `conversation.pause()`, and
Resume the paused conversation after performing operations by calling `conversation.run()` again.

```python icon="python" focus={9, 15} wrap theme={null}
import time
thread = threading.Thread(target=conversation.run)
thread.start()

print("Letting agent work for 5 seconds...")
time.sleep(5)

print("Pausing the agent...")
conversation.pause()

print("Waiting for 5 seconds...")
time.sleep(5)

print("Resuming the execution...")
conversation.run()
```

## Ready-to-run Example

<Note>
  This example is available on GitHub: [examples/01\_standalone\_sdk/09\_pause\_example.py](https://github.com/OpenHands/software-agent-sdk/blob/main/examples/01_standalone_sdk/09_pause_example.py)
</Note>

Pause agent execution mid-task by calling `conversation.pause()`:

```python icon="python" expandable examples/01_standalone_sdk/09_pause_example.py theme={null}
import os
import threading
import time

from pydantic import SecretStr

from openhands.sdk import (
    LLM,
    Agent,
    Conversation,
)
from openhands.sdk.tool import Tool
from openhands.tools.file_editor import FileEditorTool
from openhands.tools.terminal import TerminalTool


# Configure LLM
api_key = os.getenv("LLM_API_KEY")
assert api_key is not None, "LLM_API_KEY environment variable is not set."
model = os.getenv("LLM_MODEL", "anthropic/claude-sonnet-4-5-20250929")
base_url = os.getenv("LLM_BASE_URL")
llm = LLM(
    usage_id="agent",
    model=model,
    base_url=base_url,
    api_key=SecretStr(api_key),
)

# Tools
tools = [
    Tool(
        name=TerminalTool.name,
    ),
    Tool(name=FileEditorTool.name),
]

# Agent
agent = Agent(llm=llm, tools=tools)
conversation = Conversation(agent, workspace=os.getcwd())

print("=" * 60)
print("Pause and Continue Example")
print("=" * 60)
print()

# Phase 1: Start a long-running task
print("Phase 1: Starting agent with a task...")
conversation.send_message(
    "Create a file called countdown.txt and write numbers from 100 down to 1, "
    "one number per line. After you finish, summarize what you did."
)

print(f"Initial status: {conversation.state.execution_status}")
print()

# Start the agent in a background thread
thread = threading.Thread(target=conversation.run)
thread.start()

# Let the agent work for a few seconds
print("Letting agent work for 2 seconds...")
time.sleep(2)

# Phase 2: Pause the agent
print()
print("Phase 2: Pausing the agent...")
conversation.pause()

# Wait for the thread to finish (it will stop when paused)
thread.join()

print(f"Agent status after pause: {conversation.state.execution_status}")
print()

# Phase 3: Send a new message while paused
print("Phase 3: Sending a new message while agent is paused...")
conversation.send_message(
    "Actually, stop working on countdown.txt. Instead, create a file called "
    "hello.txt with just the text 'Hello, World!' in it."
)
print()

# Phase 4: Resume the agent with .run()
print("Phase 4: Resuming agent with .run()...")
print(f"Status before resume: {conversation.state.execution_status}")

# Resume execution
conversation.run()

print(f"Final status: {conversation.state.execution_status}")

# Report cost
cost = llm.metrics.accumulated_cost
print(f"EXAMPLE_COST: {cost}")
```

You can run the example code as-is.

<Note>
  The model name should follow the [LiteLLM convention](https://models.litellm.ai/): `provider/model_name` (e.g., `anthropic/claude-sonnet-4-5-20250929`, `openai/gpt-4o`).
  The `LLM_API_KEY` should be the API key for your chosen provider.
</Note>

<CodeGroup>
  <CodeBlock language="bash" filename="Bring-your-own provider key" icon="terminal" wrap>
    {`export LLM_API_KEY="your-api-key"\nexport LLM_MODEL="anthropic/claude-sonnet-4-5-20250929"  # or openai/gpt-4o, etc.\ncd software-agent-sdk\nuv run python ${path_to_script_0}`}
  </CodeBlock>

  <CodeBlock language="bash" filename="OpenHands Cloud" icon="terminal" wrap>
    {`# https://app.all-hands.dev/settings/api-keys\nexport LLM_API_KEY="your-openhands-api-key"\nexport LLM_MODEL="openhands/claude-sonnet-4-5-20250929"\ncd software-agent-sdk\nuv run python ${path_to_script_0}`}
  </CodeBlock>
</CodeGroup>

<Tip>
  **ChatGPT Plus/Pro subscribers**: You can use `LLM.subscription_login()` to authenticate with your ChatGPT account and access Codex models without consuming API credits. See the [LLM Subscriptions guide](/sdk/guides/llm-subscriptions) for details.
</Tip>

## Next Steps

* **[Persistence](/sdk/guides/convo-persistence)** - Save and restore conversation state
* **[Send Message While Processing](/sdk/guides/convo-send-message-while-running)** - Interrupt running agents
